{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport warnings\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-14T08:36:32.378232Z","iopub.execute_input":"2022-08-14T08:36:32.379064Z","iopub.status.idle":"2022-08-14T08:36:40.333464Z","shell.execute_reply.started":"2022-08-14T08:36:32.378925Z","shell.execute_reply":"2022-08-14T08:36:40.332116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read in images and create a dataframe of image paths and class labels","metadata":{}},{"cell_type":"code","source":"train_dir=r'../input/leaf-disease-detection-dataset/dataset/train'\ntest_dir=r'../input/leaf-disease-detection-dataset/dataset/test'\nfor d in [train_dir, test_dir]:\n    filepaths = []\n    labels=[] \n    classlist=sorted(os.listdir(d))\n    for klass in classlist:\n        label=klass.split('__')[1]\n        classpath=os.path.join(d, klass)\n        flist=sorted(os.listdir(classpath))\n        for f in flist:\n            fpath=os.path.join(classpath,f)\n            filepaths.append(fpath)            \n            labels.append(label)\n    Fseries=pd.Series(filepaths, name='filepaths')\n    Lseries=pd.Series(labels, name='labels')        \n    if d == train_dir:\n        df=pd.concat([Fseries, Lseries], axis=1)\n    else:\n        test_df=pd.concat([Fseries, Lseries], axis=1)\ntrain_df, test_df=train_test_split(df, train_size=.9, shuffle=True, random_state=123, stratify=df['labels'])   \nprint('train_df lenght: ', len(train_df), '  test_df length: ', len(test_df), '  test_df length: ', len(test_df))\n# get the number of classes and the images count for each class in train_df\nclasses=sorted(list(train_df['labels'].unique()))\nclass_count = len(classes)\nprint('The number of classes in the dataset is: ', class_count)\ngroups=train_df.groupby('labels')\nprint('{0:^30s} {1:^13s}'.format('CLASS', 'IMAGE COUNT'))\ncountlist=[]\nclasslist=[]\nfor label in sorted(list(train_df['labels'].unique())):\n    group=groups.get_group(label)\n    countlist.append(len(group))\n    classlist.append(label)\n    print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n\n# get the classes with the minimum and maximum number of train images\nmax_value=np.max(countlist)\nmax_index=countlist.index(max_value)\nmax_class=classlist[max_index]\nmin_value=np.min(countlist)\nmin_index=countlist.index(min_value)\nmin_class=classlist[min_index]\nprint(max_class, ' has the most images= ',max_value, ' ', min_class, ' has the least images= ', min_value)\n# lets get the average height and width of a sample of the train images\nht=0\nwt=0\n# select 100 random samples of train_df\ntrain_df_sample=train_df.sample(n=100, random_state=123,axis=0)\nfor i in range (len(train_df_sample)):\n    fpath=train_df_sample['filepaths'].iloc[i]\n    img=plt.imread(fpath)\n    shape=img.shape\n    ht += shape[0]\n    wt += shape[1]\nprint('average height= ', ht//100, ' average width= ', wt//100, 'aspect ratio= ', ht/wt)","metadata":{"execution":{"iopub.status.busy":"2022-08-14T06:16:23.701290Z","iopub.execute_input":"2022-08-14T06:16:23.701698Z","iopub.status.idle":"2022-08-14T06:16:24.453837Z","shell.execute_reply.started":"2022-08-14T06:16:23.701665Z","shell.execute_reply":"2022-08-14T06:16:24.452815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create the train_gen, test_gen**","metadata":{}},{"cell_type":"code","source":"train_gen = ImageDataGenerator(rescale=None,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True)\n\ntest_gen = ImageDataGenerator(rescale=None,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True)\n\ntraining_set=train_gen.flow_from_directory(train_dir,\n                                               target_size=(128,128),\n                                               batch_size=32,\n                                               class_mode='categorical')\n\ntest_set=test_gen.flow_from_directory(test_dir,\n                                               target_size=(128,128),\n                                               batch_size=32,\n                                               class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2022-08-14T06:16:27.734689Z","iopub.execute_input":"2022-08-14T06:16:27.735047Z","iopub.status.idle":"2022-08-14T06:16:43.504828Z","shell.execute_reply.started":"2022-08-14T06:16:27.735018Z","shell.execute_reply":"2022-08-14T06:16:43.503695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a function to show example training images**","metadata":{}},{"cell_type":"code","source":"def show_image_samples(gen):\n    t_dict=gen.class_indices\n    classes=list(t_dict.keys())    \n    images,labels=next(gen) # get a sample batch from the generator \n    plt.figure(figsize=(20, 20))\n    length=len(labels)\n    if length<25:   #show maximum of 25 images\n        r=length\n    else:\n        r=25\n    for i in range(r):        \n        plt.subplot(5, 5, i + 1)\n        image=images[i] /255       \n        plt.imshow(image)\n        index=np.argmax(labels[i])\n        class_name=classes[index]\n        plt.title(class_name, color='blue', fontsize=12)\n        plt.axis('off')\n    plt.show()\n    \nprint(\"Images for Training......................\")\nshow_image_samples(training_set )\nprint(\"Images for Testing.......................\")\nshow_image_samples(test_set )","metadata":{"execution":{"iopub.status.busy":"2022-08-14T06:16:53.106090Z","iopub.execute_input":"2022-08-14T06:16:53.107121Z","iopub.status.idle":"2022-08-14T06:16:57.504620Z","shell.execute_reply.started":"2022-08-14T06:16:53.107073Z","shell.execute_reply":"2022-08-14T06:16:57.503639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convolutional Neural Network","metadata":{}},{"cell_type":"code","source":"#basic cnn layers\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size= (3,3), activation = 'relu',input_shape=(128,128,3)))\nmodel.add(MaxPooling2D(pool_size =(2,2,)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size= (3,3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size =(2,2,)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size= (3,3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size =(2,2,)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(96,kernel_size= (3,3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size =(2,2,)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size= (3,3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size =(2,2,)))\nmodel.add(BatchNormalization())\n\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(38, activation = 'softmax'))\n#compiling our model \nmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-08-14T06:26:45.378183Z","iopub.execute_input":"2022-08-14T06:26:45.379177Z","iopub.status.idle":"2022-08-14T06:26:45.510506Z","shell.execute_reply.started":"2022-08-14T06:26:45.379142Z","shell.execute_reply":"2022-08-14T06:26:45.509557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ModelTraining","metadata":{}},{"cell_type":"code","source":"labels=(training_set.class_indices)\nlabels2=(test_set.class_indices)\n#fitting data into our model\nfitted_model = model.fit(training_set,\n                    steps_per_epoch=375,\n                    epochs=10,\n                    validation_data = test_set,\n                    validation_steps = 125)","metadata":{"execution":{"iopub.status.busy":"2022-08-14T06:29:27.501595Z","iopub.execute_input":"2022-08-14T06:29:27.502197Z","iopub.status.idle":"2022-08-14T06:49:10.238158Z","shell.execute_reply.started":"2022-08-14T06:29:27.502162Z","shell.execute_reply":"2022-08-14T06:49:10.237193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout    \n    plt.show()\n    \ntr_plot(fitted_model,0)","metadata":{"execution":{"iopub.status.busy":"2022-08-14T06:49:14.567676Z","iopub.execute_input":"2022-08-14T06:49:14.568297Z","iopub.status.idle":"2022-08-14T06:49:14.996715Z","shell.execute_reply.started":"2022-08-14T06:49:14.568257Z","shell.execute_reply":"2022-08-14T06:49:14.995437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-14T07:22:27.327795Z","iopub.execute_input":"2022-08-14T07:22:27.328372Z","iopub.status.idle":"2022-08-14T07:22:27.353167Z","shell.execute_reply.started":"2022-08-14T07:22:27.328331Z","shell.execute_reply":"2022-08-14T07:22:27.352277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Label assignment\nlabel=['Apple___Apple_scab','Apple___Black_rot','Apple___Cedar_apple_rust','Apple___healthy',\n       'Blueberry___healthy','Cherry_(including_sour)___healthy','Cherry_(including_sour)___Powdery_mildew',\n       'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot','Corn_(maize)___Common_rust_',\n       'Corn_(maize)___healthy','Corn_(maize)___Northern_Leaf_Blight','Grape___Black_rot','Grape___Esca_(Black_Measles)',\n       'Grape___healthy','Grape___Leaf_blight_(Isariopsis_Leaf_Spot)','Orange___Haunglongbing_(Citrus_greening)','Peach___Bacterial_spot',\n       'Peach___healthy','Pepper,_bell___Bacterial_spot','Pepper,_bell___healthy','Potato___Early_blight',\n       'Potato___healthy','Potato___Late_blight','Raspberry___healthy','Soybean___healthy',\n       'Squash___Powdery_mildew','Strawberry___healthy','Strawberry___Leaf_scorch','Tomato___Bacterial_spot',\n       'Tomato___Early_blight','Tomato___healthy','Tomato___Late_blight','Tomato___Leaf_Mold',\n       'Tomato___Septoria_leaf_spot','Tomato___Spider_mites Two-spotted_spider_mite','Tomato___Target_Spot',\n       'Tomato___Tomato_mosaic_virus','Tomato___Tomato_Yellow_Leaf_Curl_Virus']","metadata":{"execution":{"iopub.status.busy":"2022-08-14T07:49:10.746276Z","iopub.execute_input":"2022-08-14T07:49:10.746861Z","iopub.status.idle":"2022-08-14T07:49:10.753584Z","shell.execute_reply.started":"2022-08-14T07:49:10.746827Z","shell.execute_reply":"2022-08-14T07:49:10.752267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Output","metadata":{}},{"cell_type":"code","source":"#path=input(\"Enter your image path-: \")\ndef testing(path):\n    test_image=image.load_img(path,target_size=(128,128))\n    #print(test_image)\n    test_image=image.img_to_array(test_image)\n    test_image=np.expand_dims(test_image,axis=0)\n    result = model.predict(test_image)\n\n    #print(f\"Result is --> {result}\")\n    fresult=np.max(result)\n    label2=label[result.argmax()]\n    print(f\"your leaf disease is --> {label2}\")\n#testing(input(\"Enter your image path-: \"))\npath='../input/leaf-disease-detection-dataset/images_for_test/CornCommonRust2.JPG'\ntesting(path)","metadata":{"execution":{"iopub.status.busy":"2022-08-14T09:21:12.437594Z","iopub.execute_input":"2022-08-14T09:21:12.438025Z","iopub.status.idle":"2022-08-14T09:21:12.795257Z","shell.execute_reply.started":"2022-08-14T09:21:12.437991Z","shell.execute_reply":"2022-08-14T09:21:12.793797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* **If you find this helpful, I would really appreciate the upvote!**\n\n* **If you see something wrong please let me know.**\n\n* **And lastly Im happy to hear your thoughts about the notebook for me to also improve!**","metadata":{}}]}